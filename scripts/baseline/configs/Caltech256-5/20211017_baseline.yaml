# Template train config
experiment:
  name: "20211017_Caltech256-5_baseline"
  start_epoch: 1
  total_epochs: 300
  log_fname: "train.log"
  resume: False
  resume_fpath: ""
  save_period: 10
  eval_period: 1  # if 0, remove validate process when training.

train_transform:
  script_path: "data_loader.transform.transform"
  name: "BaseTransform"
  param:
    phase: "train"
    resize: [224, 224]

val_transform:
  script_path: "data_loader.transform.transform"
  name: "BaseTransform"
  param:
    phase: "test"
    resize: [224, 224]

train_dataset:
  name: "Caltech256-5-Trainset"
  param:
    data_root: "Caltech/Caltech256-5/train"
    train: True
    img_lst_fpath: "None"
    map_fpath: "None"
    imb_type: 'None'
    imb_factor: 0
    rand_seed: 0
    download: False

val_dataset:
  name: "Caltech256-5-Testset"
  param:
    data_root: "Caltech/Caltech256-5/test"
    train: False
    img_lst_fpath: "None"
    map_fpath: "None" # default: same as train_dataset
    download: False

trainloader:
  name: "Dataloader"
  param:
    sampler: None
    batch_size: 64
    num_workers: 8

valloader:
  name: "Dataloader"
  param:
    batch_size: 64
    num_workers: 8

network:
  name: "resnet34"
  param:
    num_classes: 5
    pretrained: True
    pretrained_fpath: ""

loss:
  name: "CrossEntropyLoss"
  param:
    a: 0.1

optimizer:
  name: "SGD"
  param:
    lr: 0.1
    weight_decay: 5.0e-4
    momentum: 0.9
    nesterov: True

warmup_lr_scheduler:
  warmup: True
  param:
    warmup_epochs: 10
    multiplier: 1

lr_scheduler:
  # name: "CyclicLR"
  # name: "StepLR"
  name: "MultiStepLR"
  # name: "CosineAnnealingWarmRestarts"
  param:
    # CyclicLR
    # step_size_up: 3  # 3 * iterations
    # step_size_down: 2  # 2 * iterations
    # max_lr: 5.0e-2
    # base_lr: 5.0e-7
    
    # MultiStepLR
    milestones: [100, 160, 200, 240, 280]
    gamma: 0.1

    # CosineAnnealingWarmRestarts
    # T_0: 5
    # T_mult: 1
    # eta_min: 0
